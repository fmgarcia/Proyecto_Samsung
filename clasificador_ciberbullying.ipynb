{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importación de Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Configuración de visualización\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Descargas necesarias de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f66c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Carga de Datos\n",
    "# Asegúrate de que la ruta al archivo es correcta\n",
    "file_path = './Dataset/cyberbullying_tweets.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset cargado con éxito. Dimensiones: {df.shape}\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo en {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2273ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de la distribución de clases\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='cyberbullying_type', data=df, palette='viridis')\n",
    "plt.title('Distribución de Tipos de Ciberbullying')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocesamiento de Texto\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Cambiar a 'spanish' si usas el dataset traducido\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = str(text).lower()\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenización y eliminación de stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # Lematización\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "print(\"Limpiando textos...\")\n",
    "df['clean_text'] = df['tweet_text'].apply(clean_text)\n",
    "print(\"Limpieza completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20650748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de datos (Train/Test)\n",
    "X = df['clean_text']\n",
    "y = df['cyberbullying_type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Datos de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Datos de prueba: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d96e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Definición de Modelos (El Array de Algoritmos)\n",
    "\n",
    "# Aquí definimos la lista de modelos que queremos probar.\n",
    "# Usamos Pipelines para incluir la vectorización (TF-IDF) como parte del modelo.\n",
    "\n",
    "models_to_evaluate = [\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "            ('clf', LogisticRegression(max_iter=1000, n_jobs=-1))\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'Linear SVC',\n",
    "        'model': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "            ('clf', LinearSVC())\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'Multinomial Naive Bayes',\n",
    "        'model': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "            ('clf', MultinomialNB())\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "            ('clf', RandomForestClassifier(n_estimators=100, n_jobs=-1))\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree',\n",
    "        'model': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "            ('clf', DecisionTreeClassifier())\n",
    "        ])\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Modelos configurados para evaluación: {len(models_to_evaluate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Bucle de Entrenamiento y Evaluación\n",
    "\n",
    "results = []\n",
    "\n",
    "for entry in models_to_evaluate:\n",
    "    model_name = entry['name']\n",
    "    model = entry['model']\n",
    "    \n",
    "    print(f\"\\n--- Entrenando {model_name} ---\")\n",
    "    \n",
    "    # Entrenamiento\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicción\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': acc,\n",
    "        'F1_Score': f1,\n",
    "        'y_pred': y_pred # Guardamos predicciones para matrices de confusión si se necesita\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualización Comparativa de Métricas\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Gráfico de Barras para Accuracy y F1 Score\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Accuracy', y='Model', data=results_df, palette='viridis')\n",
    "plt.title('Comparación de Accuracy por Modelo')\n",
    "plt.xlim(0, 1.0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='F1_Score', y='Model', data=results_df, palette='magma')\n",
    "plt.title('Comparación de F1 Score (Weighted) por Modelo')\n",
    "plt.xlim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63718e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Matriz de Confusión del Mejor Modelo\n",
    "\n",
    "# Identificar el mejor modelo basado en F1 Score\n",
    "best_model_row = results_df.loc[results_df['F1_Score'].idxmax()]\n",
    "best_model_name = best_model_row['Model']\n",
    "best_y_pred = best_model_row['y_pred']\n",
    "\n",
    "print(f\"El mejor modelo fue: {best_model_name}\")\n",
    "\n",
    "# Generar Matriz de Confusión\n",
    "cm = confusion_matrix(y_test, best_y_pred)\n",
    "labels = sorted(y.unique())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'Matriz de Confusión - {best_model_name}')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Realidad')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporte detallado del mejor modelo\n",
    "print(f\"Reporte de Clasificación para {best_model_name}:\\n\")\n",
    "print(classification_report(y_test, best_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
